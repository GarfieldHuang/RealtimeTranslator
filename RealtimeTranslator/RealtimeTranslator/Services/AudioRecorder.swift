//
//  AudioRecorder.swift
//  RealtimeTranslator
//
//  éŸ³è¨ŠéŒ„è£½æœå‹™
//

import Foundation
import AVFoundation

/// éŸ³è¨ŠéŒ„è£½é¡åˆ¥
class AudioRecorder: NSObject {
    // MARK: - å±¬æ€§

    /// éŸ³è¨Šå¼•æ“
    private var audioEngine: AVAudioEngine?

    /// è¼¸å…¥ç¯€é»
    private var inputNode: AVAudioInputNode?

    /// æ˜¯å¦æ­£åœ¨éŒ„éŸ³
    private(set) var isRecording = false

    /// ç›®æ¨™æ¡æ¨£ç‡ï¼ˆ24kHzï¼‰
    private let targetSampleRate: Double = 24000.0

    /// ç›®æ¨™è²é“æ•¸ï¼ˆå–®è²é“ï¼‰
    private let targetChannels: AVAudioChannelCount = 1

    // MARK: - å›èª¿

    /// éŸ³è¨Šè³‡æ–™å¯ç”¨æ™‚çš„å›èª¿ï¼ˆåŒ…å«éŸ³è¨Šæ•¸æ“šå’ŒéŸ³é‡è³‡è¨Šï¼‰
    var onAudioDataAvailable: ((Data, Float) -> Void)?

    /// éŒ„éŸ³ç‹€æ…‹è®Šæ›´å›èª¿
    var onRecordingStateChanged: ((Bool) -> Void)?

    /// éŒ¯èª¤å›èª¿
    var onError: ((Error) -> Void)?

    // MARK: - åˆå§‹åŒ–

    override init() {
        super.init()
    }

    // MARK: - å…¬é–‹æ–¹æ³•

    /// è«‹æ±‚éº¥å…‹é¢¨æ¬Šé™
    /// - Parameter completion: æ¬Šé™è«‹æ±‚å®Œæˆå›èª¿
    func requestMicrophonePermission(completion: @escaping (Bool) -> Void) {
        #if os(iOS)
        if #available(iOS 17.0, *) {
            AVAudioApplication.requestRecordPermission { granted in
                DispatchQueue.main.async {
                    completion(granted)
                }
            }
        } else {
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                DispatchQueue.main.async {
                    completion(granted)
                }
            }
        }
        #else
        // é iOS å¹³å°é è¨­å…è¨±
        DispatchQueue.main.async {
            completion(true)
        }
        #endif
    }

    /// é–‹å§‹éŒ„éŸ³
    /// - Throws: éŒ„éŸ³ç›¸é—œéŒ¯èª¤
    func startRecording() throws {
        guard !isRecording else {
            print("âš ï¸ å·²ç¶“åœ¨éŒ„éŸ³ä¸­")
            return
        }

        // è¨­å®šéŸ³è¨Š Session
        try setupAudioSession()

        // å»ºç«‹éŸ³è¨Šå¼•æ“
        audioEngine = AVAudioEngine()
        guard let audioEngine = audioEngine else {
            throw NSError(domain: "AudioRecorder", code: -1, userInfo: [NSLocalizedDescriptionKey: "ç„¡æ³•å»ºç«‹éŸ³è¨Šå¼•æ“"])
        }

        inputNode = audioEngine.inputNode

        // å–å¾—è¼¸å…¥æ ¼å¼
        let inputFormat = inputNode?.inputFormat(forBus: 0)
        guard let inputFormat = inputFormat else {
            throw NSError(domain: "AudioRecorder", code: -2, userInfo: [NSLocalizedDescriptionKey: "ç„¡æ³•å–å¾—éŸ³è¨Šæ ¼å¼"])
        }

        print("ğŸ“Š åŸå§‹éŸ³è¨Šæ ¼å¼:")
        print("   æ¡æ¨£ç‡: \(inputFormat.sampleRate) Hz")
        print("   è²é“æ•¸: \(inputFormat.channelCount)")
        print("   ä½å…ƒæ·±åº¦: \(inputFormat.streamDescription.pointee.mBitsPerChannel)")

        // å»ºç«‹ç›®æ¨™æ ¼å¼ï¼ˆPCM16, 24kHz, Monoï¼‰
        guard let targetFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: targetSampleRate,
            channels: targetChannels,
            interleaved: true
        ) else {
            throw NSError(domain: "AudioRecorder", code: -3, userInfo: [NSLocalizedDescriptionKey: "ç„¡æ³•å»ºç«‹ç›®æ¨™éŸ³è¨Šæ ¼å¼"])
        }

        // å»ºç«‹æ ¼å¼è½‰æ›å™¨
        guard let converter = AVAudioConverter(from: inputFormat, to: targetFormat) else {
            throw NSError(domain: "AudioRecorder", code: -4, userInfo: [NSLocalizedDescriptionKey: "ç„¡æ³•å»ºç«‹éŸ³è¨Šè½‰æ›å™¨"])
        }

        // å®‰è£éŸ³è¨Š tap
        // ä½¿ç”¨è¼ƒå°çš„ buffer size ä»¥é™ä½å»¶é²ï¼ˆ1024 samples â‰ˆ 21ms @ 48kHzï¼‰
        let bufferSize: AVAudioFrameCount = 1024

        inputNode?.installTap(onBus: 0, bufferSize: bufferSize, format: inputFormat) { [weak self] buffer, time in
            guard let self = self else { return }

            // è¨ˆç®—éŸ³é‡ï¼ˆä½¿ç”¨åŸå§‹ bufferï¼‰
            let volume = AudioProcessor.calculateVolume(buffer)

            // è½‰æ›éŸ³è¨Šæ ¼å¼
            if let convertedData = self.convertAudioBuffer(buffer, using: converter, targetFormat: targetFormat) {
                self.onAudioDataAvailable?(convertedData, volume)
            }
        }

        // å•Ÿå‹•éŸ³è¨Šå¼•æ“
        try audioEngine.start()

        isRecording = true
        onRecordingStateChanged?(true)
        print("âœ… é–‹å§‹éŒ„éŸ³")
    }

    /// åœæ­¢éŒ„éŸ³
    func stopRecording() {
        guard isRecording else {
            print("âš ï¸ ç›®å‰æœªåœ¨éŒ„éŸ³")
            return
        }

        inputNode?.removeTap(onBus: 0)
        audioEngine?.stop()
        audioEngine = nil
        inputNode = nil

        isRecording = false
        onRecordingStateChanged?(false)
        print("â¹ï¸ åœæ­¢éŒ„éŸ³")
    }

    // MARK: - ç§æœ‰æ–¹æ³•

    /// è¨­å®šéŸ³è¨Š Session
    private func setupAudioSession() throws {
        #if os(iOS)
        let audioSession = AVAudioSession.sharedInstance()

        try audioSession.setCategory(.record, mode: .measurement, options: [])
        try audioSession.setActive(true)

        print("âœ… éŸ³è¨Š Session è¨­å®šå®Œæˆ")
        #endif
    }

    /// è½‰æ›éŸ³è¨Š buffer æ ¼å¼
    private func convertAudioBuffer(
        _ buffer: AVAudioPCMBuffer,
        using converter: AVAudioConverter,
        targetFormat: AVAudioFormat
    ) -> Data? {
        // è¨ˆç®—è½‰æ›å¾Œçš„ frame æ•¸é‡
        let ratio = targetFormat.sampleRate / buffer.format.sampleRate
        let capacity = AVAudioFrameCount(Double(buffer.frameLength) * ratio)

        // å»ºç«‹è¼¸å‡º buffer
        guard let convertedBuffer = AVAudioPCMBuffer(
            pcmFormat: targetFormat,
            frameCapacity: capacity
        ) else {
            return nil
        }

        var error: NSError?

        // åŸ·è¡Œè½‰æ›
        let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }

        converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)

        if let error = error {
            print("âŒ éŸ³è¨Šè½‰æ›éŒ¯èª¤: \(error.localizedDescription)")
            return nil
        }

        // è½‰æ›ç‚º Data
        guard let channelData = convertedBuffer.int16ChannelData else {
            return nil
        }

        let channelDataPointer = channelData[0]
        let channelDataArray = Array(UnsafeBufferPointer(
            start: channelDataPointer,
            count: Int(convertedBuffer.frameLength)
        ))

        return Data(bytes: channelDataArray, count: channelDataArray.count * MemoryLayout<Int16>.size)
    }

    // MARK: - æ¸…ç†

    deinit {
        stopRecording()
    }
}
