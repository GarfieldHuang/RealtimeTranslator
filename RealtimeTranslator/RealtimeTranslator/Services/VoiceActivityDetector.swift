//
//  VoiceActivityDetector.swift
//  RealtimeTranslator
//
//  èªéŸ³æ´»å‹•æª¢æ¸¬å™¨ï¼ˆä½¿ç”¨ Speech Frameworkï¼‰
//

import Foundation
import Speech
import AVFoundation

/// èªéŸ³æ´»å‹•æª¢æ¸¬å™¨
/// ä½¿ç”¨ iOS Speech Framework é€²è¡Œé«˜ç²¾åº¦çš„äººè²æª¢æ¸¬
class VoiceActivityDetector {
    
    // MARK: - å›èª¿
    
    /// æª¢æ¸¬åˆ°é–‹å§‹èªªè©±
    var onSpeechStarted: (() -> Void)?
    
    /// æª¢æ¸¬åˆ°åœæ­¢èªªè©±
    var onSpeechEnded: (() -> Void)?
    
    /// æª¢æ¸¬åˆ°èªéŸ³ç‰‡æ®µï¼ˆå³æ™‚è­˜åˆ¥çµæœï¼Œåƒ…ç”¨æ–¼èª¿è©¦ï¼‰
    var onPartialResult: ((String) -> Void)?
    
    // MARK: - ç§æœ‰å±¬æ€§
    
    /// èªéŸ³è­˜åˆ¥å™¨
    private let speechRecognizer: SFSpeechRecognizer?
    
    /// è­˜åˆ¥è«‹æ±‚
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    
    /// è­˜åˆ¥ä»»å‹™
    private var recognitionTask: SFSpeechRecognitionTask?
    
    /// éŸ³è¨Šå¼•æ“
    private let audioEngine = AVAudioEngine()
    
    /// æ˜¯å¦æ­£åœ¨è­˜åˆ¥
    private var isRecognizing = false
    
    /// æ˜¯å¦æª¢æ¸¬åˆ°èªéŸ³æ´»å‹•
    private var isSpeechActive = false
    
    /// æœ€å¾Œä¸€æ¬¡æª¢æ¸¬åˆ°èªéŸ³çš„æ™‚é–“
    private var lastSpeechTime: Date?
    
    /// ç„¡èªéŸ³è¨ˆæ™‚å™¨
    private var silenceTimer: Timer?
    
    // MARK: - å¯é…ç½®åƒæ•¸
    
    /// éœé»˜æª¢æ¸¬æ™‚é–“ï¼ˆç§’ï¼‰- è¶…éæ­¤æ™‚é–“æ²’æœ‰èªéŸ³å‰‡èªç‚ºèªªè©±çµæŸ
    var silenceThreshold: TimeInterval = 1.0
    
        /// æœ€çŸ­èªéŸ³é•·åº¦ï¼ˆç§’ï¼‰- å°æ–¼æ­¤é•·åº¦çš„èªéŸ³ç‰‡æ®µæœƒè¢«å¿½ç•¥ï¼ˆé¿å…é›œéŸ³èª¤è§¸ï¼‰
    var minimumSpeechDuration: TimeInterval = 0.05
    
    /// èªéŸ³é–‹å§‹çš„å»¶é²å®¹éŒ¯ï¼ˆç§’ï¼‰- é–‹å§‹èªªè©±å¾Œå®¹è¨±çš„å‰ç½®æ™‚é–“
    var speechStartDelay: TimeInterval = 0.2
    
    /// èªéŸ³é–‹å§‹æ™‚é–“
    private var speechStartTime: Date?
    
    // MARK: - åˆå§‹åŒ–
    
    /// åˆå§‹åŒ–
    /// - Parameter locale: èªè¨€å€åŸŸï¼Œç”¨æ–¼å„ªåŒ–è­˜åˆ¥æº–ç¢ºåº¦
    init(locale: Locale = Locale(identifier: "zh-TW")) {
        // ä½¿ç”¨æŒ‡å®šèªè¨€çš„èªéŸ³è­˜åˆ¥å™¨
        self.speechRecognizer = SFSpeechRecognizer(locale: locale)
        
        // è¨­å®šè­˜åˆ¥å™¨ç‚ºè¨­å‚™ç«¯è­˜åˆ¥ï¼ˆæ›´å¿«ã€æ›´çœé›»ã€ä¿è­·éš±ç§ï¼‰
        if #available(iOS 13.0, *) {
            speechRecognizer?.supportsOnDeviceRecognition = true
        }
    }
    
    // MARK: - å…¬é–‹æ–¹æ³•
    
    /// è«‹æ±‚èªéŸ³è­˜åˆ¥æ¬Šé™
    /// - Parameter completion: å®Œæˆå›èª¿ï¼Œè¿”å›æ˜¯å¦æˆæ¬Š
    func requestAuthorization(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("âœ… èªéŸ³è­˜åˆ¥æ¬Šé™å·²æˆäºˆ")
                    completion(true)
                case .denied:
                    print("âŒ èªéŸ³è­˜åˆ¥æ¬Šé™è¢«æ‹’çµ•")
                    completion(false)
                case .restricted:
                    print("âš ï¸ èªéŸ³è­˜åˆ¥æ¬Šé™å—é™")
                    completion(false)
                case .notDetermined:
                    print("âš ï¸ èªéŸ³è­˜åˆ¥æ¬Šé™æœªç¢ºå®š")
                    completion(false)
                @unknown default:
                    print("âš ï¸ æœªçŸ¥çš„èªéŸ³è­˜åˆ¥æ¬Šé™ç‹€æ…‹")
                    completion(false)
                }
            }
        }
    }
    
    /// é–‹å§‹æª¢æ¸¬
    func startDetecting() throws {
        // ç¢ºä¿æœ‰èªéŸ³è­˜åˆ¥å™¨
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            throw NSError(domain: "VoiceActivityDetector", code: -1, 
                         userInfo: [NSLocalizedDescriptionKey: "èªéŸ³è­˜åˆ¥å™¨ä¸å¯ç”¨"])
        }
        
        // å¦‚æœæ­£åœ¨è­˜åˆ¥ï¼Œå…ˆåœæ­¢
        if isRecognizing {
            stopDetecting()
        }
        
        // æº–å‚™éŸ³è¨Šæœƒè©±
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        // å‰µå»ºè­˜åˆ¥è«‹æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw NSError(domain: "VoiceActivityDetector", code: -2,
                         userInfo: [NSLocalizedDescriptionKey: "ç„¡æ³•å‰µå»ºè­˜åˆ¥è«‹æ±‚"])
        }
        
        // è¨­å®šç‚ºå³æ™‚è­˜åˆ¥
        recognitionRequest.shouldReportPartialResults = true
        
        // è¨­å®šç‚ºè¨­å‚™ç«¯è­˜åˆ¥ï¼ˆå¦‚æœæ”¯æ´ï¼‰
        if #available(iOS 13.0, *) {
            recognitionRequest.requiresOnDeviceRecognition = true
        }
        
        // ç²å–éŸ³è¨Šè¼¸å…¥ç¯€é»
        let inputNode = audioEngine.inputNode
        
        // é–‹å§‹è­˜åˆ¥ä»»å‹™
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                self.handleRecognitionResult(result)
            }
            
            if error != nil || result?.isFinal == true {
                self.handleRecognitionEnd()
            }
        }
        
        // è¨­å®šéŸ³è¨Šæ ¼å¼å’Œå®‰è£ tap
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        // æº–å‚™ä¸¦å•Ÿå‹•éŸ³è¨Šå¼•æ“
        audioEngine.prepare()
        try audioEngine.start()
        
        isRecognizing = true
        isSpeechActive = false
        speechStartTime = nil
        lastSpeechTime = nil
        
        print("ğŸ™ï¸ VAD é–‹å§‹æª¢æ¸¬äººè²")
    }
    
    /// åœæ­¢æª¢æ¸¬
    func stopDetecting() {
        // åœæ­¢éŸ³è¨Šå¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // çµæŸè­˜åˆ¥è«‹æ±‚
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // å–æ¶ˆè­˜åˆ¥ä»»å‹™
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // å–æ¶ˆè¨ˆæ™‚å™¨
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        // å¦‚æœé‚„åœ¨èªªè©±ç‹€æ…‹ï¼Œè§¸ç™¼çµæŸå›èª¿
        if isSpeechActive {
            notifySpeechEnded()
        }
        
        isRecognizing = false
        
        print("ğŸ›‘ VAD åœæ­¢æª¢æ¸¬")
    }
    
    /// æ›´æ–°èªè¨€è¨­å®š
    /// - Parameter locale: æ–°çš„èªè¨€å€åŸŸ
    func updateLocale(_ locale: Locale) {
        // éœ€è¦é‡æ–°åˆå§‹åŒ–è­˜åˆ¥å™¨
        let wasRecognizing = isRecognizing
        
        if wasRecognizing {
            stopDetecting()
        }
        
        // æ³¨æ„ï¼šé€™è£¡éœ€è¦é‡æ–°å‰µå»º VoiceActivityDetector å¯¦ä¾‹
        // å› ç‚º SFSpeechRecognizer åœ¨åˆå§‹åŒ–å¾Œç„¡æ³•æ›´æ”¹ locale
        print("âš ï¸ èªè¨€è®Šæ›´éœ€è¦é‡æ–°å‰µå»º VAD å¯¦ä¾‹")
    }
    
    // MARK: - ç§æœ‰æ–¹æ³•
    
    /// è™•ç†è­˜åˆ¥çµæœ
    private func handleRecognitionResult(_ result: SFSpeechRecognitionResult) {
        let transcription = result.bestTranscription.formattedString
        
        // å¦‚æœæœ‰è­˜åˆ¥åˆ°æ–‡å­—ï¼Œè¡¨ç¤ºæœ‰èªéŸ³æ´»å‹•
        if !transcription.isEmpty {
            lastSpeechTime = Date()
            
            // å¦‚æœä¹‹å‰æ²’æœ‰æª¢æ¸¬åˆ°èªéŸ³ï¼Œç¾åœ¨æª¢æ¸¬åˆ°äº†
            if !isSpeechActive {
                speechStartTime = Date()
                isSpeechActive = true
                
                // å»¶é²ä¸€é»é»å†é€šçŸ¥ï¼ˆé¿å…èª¤è§¸ç™¼ï¼‰
                DispatchQueue.main.asyncAfter(deadline: .now() + speechStartDelay) { [weak self] in
                    guard let self = self, self.isSpeechActive else { return }
                    self.notifySpeechStarted()
                }
            }
            
            // é‡ç½®éœé»˜è¨ˆæ™‚å™¨
            resetSilenceTimer()
            
            // å¯é¸ï¼šå›å‚³éƒ¨åˆ†è­˜åˆ¥çµæœï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
            #if DEBUG
            onPartialResult?(transcription)
            #endif
        }
    }
    
    /// è™•ç†è­˜åˆ¥çµæŸ
    private func handleRecognitionEnd() {
        if isSpeechActive {
            notifySpeechEnded()
        }
    }
    
    /// é‡ç½®éœé»˜è¨ˆæ™‚å™¨
    private func resetSilenceTimer() {
        silenceTimer?.invalidate()
        
        silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceThreshold, repeats: false) { [weak self] _ in
            guard let self = self else { return }
            
            if self.isSpeechActive {
                // æª¢æŸ¥èªéŸ³é•·åº¦æ˜¯å¦ç¬¦åˆæœ€çŸ­è¦æ±‚
                if let startTime = self.speechStartTime {
                    let duration = Date().timeIntervalSince(startTime)
                    if duration >= self.minimumSpeechDuration {
                        self.notifySpeechEnded()
                    } else {
                        print("â­ï¸ èªéŸ³ç‰‡æ®µå¤ªçŸ­ (\(String(format: "%.2f", duration))ç§’)ï¼Œå¿½ç•¥")
                        self.isSpeechActive = false
                        self.speechStartTime = nil
                    }
                } else {
                    self.notifySpeechEnded()
                }
            }
        }
    }
    
    /// é€šçŸ¥é–‹å§‹èªªè©±
    private func notifySpeechStarted() {
        print("ğŸ—£ï¸ VAD æª¢æ¸¬åˆ°é–‹å§‹èªªè©±")
        DispatchQueue.main.async { [weak self] in
            self?.onSpeechStarted?()
        }
    }
    
    /// é€šçŸ¥åœæ­¢èªªè©±
    private func notifySpeechEnded() {
        print("ğŸ¤ VAD æª¢æ¸¬åˆ°åœæ­¢èªªè©±")
        isSpeechActive = false
        speechStartTime = nil
        
        DispatchQueue.main.async { [weak self] in
            self?.onSpeechEnded?()
        }
    }
    
    // MARK: - Deinit
    
    deinit {
        stopDetecting()
    }
}
